# Week 5 Implementation Guide: Production Optimization & MLX Model Integration

## ðŸŽ¯ Overview
Week 5 focuses on completing the production-ready deployment by implementing actual MLX model inference, Docker containerization, comprehensive testing, and documentation. This final week transforms our working prototype into a deployable service.

## ðŸ“Š Current Status (Week 4 Complete)
âœ… **Achieved**:
- FastAPI application with Context7 patterns
- All API endpoints working (/embed, /rerank, /health, /docs)
- Backend abstraction with MLX placeholder
- Comprehensive monitoring and logging
- Performance: ~0.4ms embedding, ~1ms reranking

ðŸŽ¯ **Week 5 Goals**:
- Replace MLX placeholder with actual model inference
- Docker containerization for production deployment
- Comprehensive test suite
- Complete documentation and deployment guides

---

## ðŸš€ Day-by-Day Implementation Plan

### Day 29-30: MLX Model Integration & Optimization

#### Priority 1: Actual MLX Model Implementation

**Current State**: MLX backend uses placeholder embeddings (random 384-dimensional vectors)
**Target**: Implement actual Qwen3-Embedding-4B inference with MLX

**Options for MLX Model Integration**:

1. **Option A: Pre-converted MLX Models** (Recommended)
   ```bash
   # Look for MLX-converted versions
   huggingface-hub download --repo-type model mlx-community/Qwen2.5-Coder-7B-Instruct-MLX
   # Or similar embedding models
   ```

2. **Option B: Manual MLX Conversion**
   ```python
   # In app/backends/mlx_backend.py
   from mlx_lm.utils import convert_transformers_to_mlx
   
   def convert_model_to_mlx(model_name: str, mlx_path: str):
       """Convert HuggingFace model to MLX format"""
       # Implementation details
   ```

3. **Option C: Direct MLX Implementation**
   ```python
   # Direct implementation using MLX operations
   import mlx.core as mx
   import mlx.nn as nn
   
   class MLXEmbeddingModel(nn.Module):
       # Custom MLX model implementation
   ```

**Implementation Steps**:

1. **Update MLXBackend Class**:
   ```python
   # app/backends/mlx_backend.py
   class MLXBackend(BaseBackend):
       async def load_model(self):
           """Load actual MLX model instead of placeholder"""
           if self.mlx_model_path and self.mlx_model_path.exists():
               # Load pre-converted MLX model
               self.model = load_mlx_model(self.mlx_model_path)
           else:
               # Try to find MLX-community version or convert
               self.model = await self._load_or_convert_model()
       
       async def _load_or_convert_model(self):
           """Load MLX model or convert from HuggingFace"""
           # Implementation logic
   ```

2. **Update generate_embeddings Method**:
   ```python
   async def generate_embeddings(self, texts: List[str], **kwargs) -> EmbeddingResult:
       """Generate real embeddings using MLX model"""
       if not self.model:
           raise RuntimeError("MLX model not loaded")
       
       # Tokenize texts
       tokens = self.tokenizer(texts, ...)
       
       # Run MLX inference
       with mx.eval():
           embeddings = self.model(tokens)
       
       return EmbeddingResult(
           vectors=np.array(embeddings),
           model_info=f"MLX:{self.model_name}"
       )
   ```

#### Priority 2: Performance Optimization

**Batch Processing Optimization**:
```python
async def _process_batch(self, batch_texts: List[str]) -> np.ndarray:
    """Optimized batch processing for MLX"""
    # Implement efficient batching
    # Memory-efficient token handling
    # Optimal inference patterns
```

**Memory Management**:
```python
def _manage_mlx_memory(self):
    """MLX-specific memory management"""
    mx.metal.clear_cache()  # Clear GPU memory
    # Implement memory monitoring
```

### Day 31-32: Docker Containerization

#### Priority 1: Multi-stage Dockerfile

**Create: Dockerfile**
```dockerfile
# Stage 1: Build stage
FROM python:3.11-slim as builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy and install requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Stage 2: Runtime stage
FROM python:3.11-slim as runtime

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Copy virtual environment from builder
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Create non-root user
RUN useradd -m -u 1000 appuser
WORKDIR /app
RUN chown appuser:appuser /app

# Copy application code
COPY --chown=appuser:appuser app/ ./app/

# Switch to non-root user
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:9000/health/ || exit 1

# Expose port
EXPOSE 9000

# Run application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "9000"]
```

#### Priority 2: Docker Compose Configuration

**Create: docker-compose.yml**
```yaml
version: '3.8'

services:
  embed-rerank-api:
    build: .
    ports:
      - "9000:9000"
    environment:
      - BACKEND=auto
      - MODEL_NAME=Qwen/Qwen3-Embedding-4B
      - LOG_LEVEL=INFO
      - HOST=0.0.0.0
      - PORT=9000
    volumes:
      - ./models:/app/models:ro  # For local model cache
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Optional: Monitoring stack
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
```

**Create: docker-compose.dev.yml**
```yaml
version: '3.8'

services:
  embed-rerank-api:
    build: 
      context: .
      target: builder  # Use build stage for development
    ports:
      - "9000:9000"
    volumes:
      - .:/app
      - /app/.venv  # Exclude venv from mount
    environment:
      - RELOAD=true
      - LOG_LEVEL=DEBUG
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "9000", "--reload"]
```

#### Priority 3: Environment Configuration

**Create: .env.production**
```env
# Production Environment Configuration
BACKEND=auto
MODEL_NAME=Qwen/Qwen3-Embedding-4B
MLX_MODEL_PATH=/app/models/mlx

# Server Configuration
HOST=0.0.0.0
PORT=9000
RELOAD=false

# Performance Settings
BATCH_SIZE=32
MAX_BATCH_SIZE=128
MAX_TEXTS_PER_REQUEST=100
MAX_PASSAGES_PER_RERANK=1000

# Security
ALLOWED_HOSTS=["*"]  # Configure for production
ALLOWED_ORIGINS=["*"]  # Configure for production

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Resource Limits
DEVICE_MEMORY_FRACTION=0.8
REQUEST_TIMEOUT=300
```

### Day 33-35: Testing & Documentation

#### Priority 1: Comprehensive Test Suite

**Create: tests/test_api_integration.py**
```python
import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

class TestAPIIntegration:
    """Integration tests for all API endpoints"""
    
    def test_health_endpoint(self):
        """Test health check endpoint"""
        response = client.get("/health/")
        assert response.status_code == 200
        data = response.json()
        assert data["status"] in ["healthy", "warning", "unhealthy"]
    
    def test_embed_endpoint(self):
        """Test embedding generation"""
        payload = {
            "texts": ["Hello world", "This is a test"],
            "batch_size": 32,
            "normalize": True
        }
        response = client.post("/api/v1/embed/", json=payload)
        assert response.status_code == 200
        data = response.json()
        assert len(data["embeddings"]) == 2
        assert data["dim"] == 384
    
    def test_rerank_endpoint(self):
        """Test passage reranking"""
        payload = {
            "query": "machine learning",
            "passages": [
                "AI and machine learning are transforming industries",
                "Dogs are pets",
                "Deep learning is a subset of machine learning"
            ],
            "top_k": 3
        }
        response = client.post("/api/v1/rerank/", json=payload)
        assert response.status_code == 200
        data = response.json()
        assert len(data["results"]) == 3
        # Results should be sorted by score
        scores = [r["score"] for r in data["results"]]
        assert scores == sorted(scores, reverse=True)
```

**Create: tests/test_performance.py**
```python
import time
import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

class TestPerformance:
    """Performance regression tests"""
    
    def test_embedding_performance(self):
        """Test embedding endpoint performance"""
        texts = ["Sample text"] * 100
        payload = {"texts": texts}
        
        start_time = time.time()
        response = client.post("/api/v1/embed/", json=payload)
        end_time = time.time()
        
        assert response.status_code == 200
        processing_time = end_time - start_time
        assert processing_time < 5.0  # Should complete within 5 seconds
    
    def test_concurrent_requests(self):
        """Test handling of concurrent requests"""
        import concurrent.futures
        
        def make_request():
            payload = {"texts": ["Test text"]}
            return client.post("/api/v1/embed/", json=payload)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(make_request) for _ in range(10)]
            responses = [f.result() for f in futures]
        
        assert all(r.status_code == 200 for r in responses)
```

**Create: tests/test_load.py**
```python
# Load testing with locust (optional)
from locust import HttpUser, task, between

class EmbedRerankUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(3)
    def test_embed(self):
        self.client.post("/api/v1/embed/", json={
            "texts": ["Sample text for load testing"]
        })
    
    @task(1)
    def test_rerank(self):
        self.client.post("/api/v1/rerank/", json={
            "query": "test query",
            "passages": ["passage 1", "passage 2", "passage 3"]
        })
    
    @task(1)
    def test_health(self):
        self.client.get("/health/")
```

#### Priority 2: Documentation Completion

**Update: README.md**
```markdown
# Embed-Rerank API

Production-ready text embedding and document reranking service optimized for Apple Silicon.

## Quick Start

### Docker (Recommended)
```bash
# Pull and run
docker pull embed-rerank:latest
docker run -p 9000:9000 embed-rerank:latest

# Or build from source
docker-compose up --build
```

### Local Development
```bash
# Setup
git clone <repo>
cd embed-rerank
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Run
python -m app.main
```

## API Usage

### Generate Embeddings
```bash
curl -X POST "http://localhost:9000/api/v1/embed/" \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Hello world", "This is a test"]}'
```

### Rerank Passages
```bash
curl -X POST "http://localhost:9000/api/v1/rerank/" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "machine learning",
    "passages": ["AI and ML transform industries", "Dogs are pets"]
  }'
```

## Performance

| Backend | Loading Time | Inference Speed | Latency |
|---------|-------------|----------------|---------|
| MLX (Apple Silicon) | 0.47s | 30,411 texts/sec | 0.001s |
| PyTorch (MPS) | 2.71s | 569 texts/sec | 0.035s |

## Configuration

See `.env.example` for all configuration options.

## Deployment

- **Docker**: Multi-stage optimized container
- **Health Checks**: Built-in monitoring endpoints
- **Logging**: Structured JSON logging
- **Security**: Non-root user, minimal dependencies
```

**Create: docs/API_GUIDE.md**
- Complete API reference
- Usage examples
- Error handling guide
- Authentication (if needed)

**Create: docs/DEPLOYMENT_GUIDE.md**
- Docker deployment
- Kubernetes configuration
- Monitoring setup
- Scaling considerations

---

## ðŸŽ¯ Success Criteria for Week 5

### Technical Milestones
- [ ] **MLX Real Model**: Actual embedding inference (not placeholder)
- [ ] **Docker Ready**: Production container builds and runs
- [ ] **Test Coverage**: >90% for critical paths
- [ ] **Documentation**: Complete user and deployment guides

### Performance Targets
- [ ] **MLX Performance**: Actual vs placeholder benchmark comparison
- [ ] **Container Size**: <2GB final image
- [ ] **Startup Time**: <30 seconds from container start to ready
- [ ] **Memory Usage**: Stable under sustained load

### Quality Gates
- [ ] **All Tests Pass**: Unit, integration, performance tests
- [ ] **Security Scan**: Container vulnerability assessment
- [ ] **Load Testing**: Handle 100 concurrent requests
- [ ] **Documentation**: Complete and accurate

---

## ðŸš¨ Risk Mitigation

### MLX Model Integration Risks
**Risk**: MLX model conversion complexity
**Mitigation**: Start with pre-converted models, fallback to conversion tools

**Risk**: Performance degradation vs placeholder
**Mitigation**: Benchmark early, optimize iteratively

### Docker Deployment Risks
**Risk**: Large container size
**Mitigation**: Multi-stage builds, minimal base images

**Risk**: Platform compatibility
**Mitigation**: Test on both Apple Silicon and x86 platforms

---

## ðŸ”§ Development Tools

### Testing Commands
```bash
# Run all tests
pytest tests/ -v

# Performance tests
pytest tests/test_performance.py -v

# Load testing
locust -f tests/test_load.py --host=http://localhost:9000
```

### Docker Commands
```bash
# Build and test
docker build -t embed-rerank:latest .
docker run -p 9000:9000 embed-rerank:latest

# Development with compose
docker-compose -f docker-compose.dev.yml up
```

### Benchmarking
```bash
# Run current benchmark suite
python -m app.utils.benchmark

# Compare before/after performance
python -m tests.benchmark_comparison
```

---

This Week 5 guide focuses on completing the production deployment with actual MLX model integration, containerization, and comprehensive testing. The goal is to deliver a fully deployable service ready for production use.
